## 梯度下降算法的更新公式

### 0.矩阵求导

在推导梯度下降算法时，理解矩阵求导法对于处理高维数据和优化问题非常有用。

假设我们有 $m$ 个样本，每个样本有 $n$ 个特征。用 $X \in \mathbb{R}^{m \times n}$ 表示输入特征矩阵，其中每一行是一个样本的特征向量。参数向量 $\theta \in \mathbb{R}^{n}$ 和目标向量 $y \in \mathbb{R}^{m}$。线性回归模型的预测值可以表示为：
$$
\hat{y} = X \theta
$$

### 1. 成本函数（MSE）

对于线性回归模型，假设我们有 $m$ 个训练样本，每个样本 $(x_i, y_i)$，其中 $x_i$ 是输入特征，$y_i$ 是实际值。线性回归模型的预测值 $h_{\theta}(x_i)$ 定义为：
$$
h_{\theta}(x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \ldots + \theta_n x_{in}
$$

我们将所有参数 $\theta$ 和输入特征 $x_i$ 以向量形式表示：
$$
h_{\theta}(x_i) = \theta^T x_i
$$

均方误差（MSE）成本函数定义为：
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x_i) - y_i)^2
$$

其中，$\hat{y} = X \theta$。用矩阵表示为：
$$
J(\theta) = \frac{1}{2m} \| X \theta - y \|^2
$$

### 2. 计算梯度

我们需要计算成本函数 $J(\theta)$ 对每个参数 $\theta_j$ 的偏导数。 

首先，成本函数对参数 $\theta_j$ 的偏导数为：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} \left( \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x_i) - y_i)^2 \right)
$$

将 $h_{\theta}(x_i)$ 展开为 $\theta^T x_i$：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{2m} \sum_{i=1}^m \frac{\partial}{\partial \theta_j} \left( (\theta^T x_i - y_i)^2 \right)
$$

应用链式法则进行求导：
$$
\frac{\partial}{\partial \theta_j} \left( (\theta^T x_i - y_i)^2 \right) = 2 (\theta^T x_i - y_i) \cdot \frac{\partial}{\partial \theta_j} (\theta^T x_i - y_i)
$$

由于 $\frac{\partial}{\partial \theta_j} (\theta^T x_i - y_i) = x_{ij}$（$x_{ij}$ 是 $x_i$ 的第 $j$ 个分量）：
$$
\frac{\partial}{\partial \theta_j} (\theta^T x_i - y_i)^2 = 2 (\theta^T x_i - y_i) \cdot x_{ij}
$$

将其代入原始偏导数表达式：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{2m} \sum_{i=1}^m 2 (\theta^T x_i - y_i) \cdot x_{ij}
$$

化简得：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (\theta^T x_i - y_i) \cdot x_{ij}
$$

### 3. 梯度下降更新公式

梯度下降算法使用梯度信息更新参数 $\theta_j$，更新公式为：
$$
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
$$

将偏导数的表达式代入更新公式：
$$
\theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^m (\theta^T x_i - y_i) \cdot x_{ij} \right)
$$

将上述公式向量化表示，可以将所有参数的更新写成矩阵形式：

### 向量化表示

其中，$\hat{y} = X \theta$。用矩阵表示为：
$$
J(\theta) = \frac{1}{2m} \| X \theta - y \|^2
$$

#### 1. 成本函数展开

首先，将成本函数展开：
$$
J(\theta) = \frac{1}{2m} (X \theta - y)^T (X \theta - y)
$$

#### 2. 使用矩阵求导规则

我们需要使用矩阵求导规则来对 $J(\theta)$ 求导。这里使用一个重要的矩阵求导公式：
$$
\frac{\partial}{\partial \theta} \left( a^T a \right) = 2A^T a \quad \text{如果} \quad a = A \theta
$$

在我们的情况下，$a = X \theta - y$，所以：
$$
\frac{\partial}{\partial \theta} \left( (X \theta - y)^T (X \theta - y) \right) = 2 X^T (X \theta - y)
$$

#### 3. 结合常数项

由于我们的成本函数前面有一个常数项 $\frac{1}{2m}$，将其乘入导数表达式中：
$$
\nabla_{\theta} J(\theta) = \frac{1}{2m} \cdot 2 X^T (X \theta - y) = \frac{1}{m} X^T (X \theta - y)
$$

有了梯度 $\nabla_{\theta} J(\theta)$，我们可以写出梯度下降的更新公式：**（α为学习率）**
$$
\theta := \theta - \alpha \nabla_{\theta} J(\theta)
$$

将梯度代入公式中：
$$
\theta := \theta - \alpha \left( \frac{1}{m} X^T (X \theta - y) \right)
$$

这就是线性回归中梯度下降算法的更新公式，使用矩阵求导的方法推导出来的。

令 $X$ 为输入矩阵，包含所有训练样本的特征，大小为 $m \times (n+1)$（$n$ 是特征数，附加一列1以表示截距项）。令 $\theta$ 为参数向量，大小为 $(n+1) \times 1$，$y$ 为实际值向量，大小为 $m \times 1$。

矩阵形式的预测值为：
$$
h_{\theta} = X \theta
$$

成本函数的梯度为：
$$
\nabla_{\theta} J(\theta) = \frac{1}{m} X^T (X \theta - y)
$$

梯度下降的向量化更新公式为：
$$
\theta := \theta - \alpha \nabla_{\theta} J(\theta)
$$

代入梯度表达式：
$$
\theta := \theta - \alpha \left( \frac{1}{m} X^T (X \theta - y) \right)
$$

#### 总结

详细推导得出的梯度下降更新公式为：
$$
\theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^m (\theta^T x_i - y_i) \cdot x_{ij} \right)
$$

向量化形式为：
$$
\theta := \theta - \alpha \left( \frac{1}{m} X^T (X \theta - y) \right)
$$

这些公式在实际应用中帮助我们通过迭代更新参数 $\theta$ 来最小化成本函数，从而优化线性回归模型。