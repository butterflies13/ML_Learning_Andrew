# Lecture 2：Liner Regression and Gradient Descent

## 回归问题（Regression Problem）

**回归问题是指一类预测问题，其中目标是预测连续数值输出的变量。**

通过输入特征x和输出标签y学习从x到y的映射，模型的目标是找到一个函数，使得对新的输入数据能够准确预测其对应的输出值。

### 预测房价

> 当我们谈论预测房价时，通常涉及回归问题。假设我们有一组数据，每条数据包含了一些房屋的特征和对应的价格。这些特征可以包括房屋的大小（平方英尺）、卧室数量、浴室数量、地理位置等。
>
> 假设我们有如下的数据集：
>
> | 房屋大小（平方英尺） | 卧室数量 | 价格（千美元） |
> | -------------------- | -------- | -------------- |
> | 2000                 | 3        | 250            |
> | 1600                 | 2        | 180            |
> | 3000                 | 4        | 350            |
> | 2400                 | 3        | 300            |
>
> 目标是建立一个模型，根据房屋的特征预测房屋的价格。这是一个典型的**回归问题**，因为价格是一个连续的数值变量。

**基本模型**

![回归模型](E:\ML_Learning-Andrew Ng\图\回归模型.jpg)

### **表示输出假设H**

#### 符号

1. Parameter(θ)：参数
2. m：# training examples
3. n：# features
4. x：input features
5. y：output target/variable
6. $x_j^{(i)}$​：第 i 个训练样本的特征 j 的值
7. $(x^{(i)},y^{(i)})$：索引为i的training example

#### 多元线性回归的假设函数

对于简单线性回归，模型可以表示为：
$$ h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \epsilon $$
其中，$ y $ 是因变量，$ x $ 是自变量，$ \beta_0 $ 是截距，$ \beta_1 $ 是斜率，$ \epsilon $ 是误差项。多元线性回归中，这个关系可以扩展为：
$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon $$

**显然x和θ为n+1维**

### 回归函数的目标

choose θ set  $h(x) \approx y$​  for train examples              

我们的目标是找到最优的参数集 θ（或回归系数），使得模型的预测值 h(x)尽可能接近训练数据中的实际值 y。为了实现这一目标，我们通常通过最小化成本函数（也称为损失函数）来达到目的。

#### 成本函数（Minimize the cost function）

为了量化预测值 $h(x_i)$ 与实际值 $y_i$ 之间的差距，我们定义了一个成本函数 $J(\theta)$。最常用的成本函数是均方误差（MSE），其定义为：
$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h(x_i) - y_i)^2 $$

其中，$m$ 是训练样本的数量。

> 均方误差（Mean Squared Error, MSE）是统计学和机器学习中常用的损失函数或成本函数，用于衡量回归模型预测值与实际值之间的差异。它通过计算预测误差的平方和，然后取平均值来评估模型的表现。MSE 越小，模型的预测精度越高。
>
> 对于一个给定的回归模型，假设我们有 $m$ 个训练样本，$x_i$ 是第 $i$ 个样本的输入特征，$y_i$ 是第 $i$ 个样本的真实输出值，$h(x_i)$ 是模型对第 $i$ 个样本的预测值。均方误差定义为：
>
> $$
> \text{MSE} = \frac{1}{m} \sum_{i=1}^m (h(x_i) - y_i)^2
> $$
>
> 1. **误差计算**：
>    - 对于每一个训练样本 $(x_i, y_i)$，计算模型的预测值 $h(x_i)$ 与实际值 $y_i$ 之间的误差：
>      $$
>      \text{误差} = h(x_i) - y_i
>      $$
>    
> 2. **平方误差**：
>    - 将每个样本的误差进行平方，以确保误差是非负数，并且对大误差进行更大的惩罚：
>      $$
>      \text{平方误差} = (h(x_i) - y_i)^2
>      $$
>    
> 3. **总和和平均**：
>    - 将所有样本的平方误差加总，然后取平均值，以得到模型在整个训练集上的均方误差：
>      $$
>      \text{MSE} = \frac{1}{m} \sum_{i=1}^m (h(x_i) - y_i)^2
>      $$
>

#### 梯度下降算法

梯度下降（Gradient Descent）是一种优化算法，常用于机器学习模型的参数调整，以最小化损失函数或成本函数。通过迭代地调整参数，梯度下降算法逐步逼近成本函数的最小值，从而找到最优的模型参数。

梯度下降的核心思想是利用成本函数的梯度信息，在参数空间中沿着梯度的负方向移动，从而减小成本函数的值。梯度的负方向指示了成本函数下降最快的方向。

**通过这样选择一个方向前进一小步，持续优化，最终找到局部最优解。可以得知，从不同的点位出发，得到的最优解可能不同。**

![image-20240721002757869](E:\ML_Learning-Andrew Ng\图\gradient descent.png)

梯度下降（Gradient Descent）是一种优化算法，常用于机器学习模型的参数调整，以最小化损失函数或成本函数。通过迭代地调整参数，梯度下降算法逐步逼近成本函数的最小值，从而找到最优的模型参数。

##### 梯度下降算法的步骤

1. **初始化参数**：
   - 随机选择或设定初始参数值（如 $\theta$）。

2. **计算梯度**：
   
   - 对于每个参数 $\theta_j$，计算成本函数 $J(\theta)$ 对 $\theta_j$ 的偏导数，即梯度：
     $$
     \frac{\partial J(\theta)}{\partial \theta_j}
     $$
   
3. **更新参数**：
   - 根据梯度信息，更新每个参数 $\theta_j$，移动步长由学习率 $\alpha$ 决定：
     $$
     \theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
     $$

4. **迭代**：
   - 重复计算梯度和更新参数的步骤，直到满足停止条件，如成本函数的变化小于某个阈值或达到预设的迭代次数。

##### 梯度下降的类型

梯度下降算法有几种不同的变体，主要根据每次迭代中使用的数据量来分类：

1. **批量梯度下降（Batch Gradient Descent）**：
   - 每次迭代使用整个训练集来计算梯度和更新参数。
   - 优点：每次更新的方向是全局的，因此收敛稳定。
   - 缺点：计算成本高，尤其是对于大数据集。
2. **随机梯度下降（Stochastic Gradient Descent, SGD）**：
   - 每次迭代使用一个随机选择的训练样本来计算梯度和更新参数。
   - 优点：计算速度快，能够快速更新参数。
   - 缺点：每次更新的方向具有较大随机性，收敛过程不稳定，可能在全局最优值附近波动。（**永远不会完全收敛，对于大数据集更快，无限接近全局最优值**）。
3. **小批量梯度下降（Mini-batch Gradient Descent）**：
   - 每次迭代使用一个小批量的训练样本（mini-batch）来计算梯度和更新参数。
   - 结合了批量梯度下降和随机梯度下降的优点，计算效率较高且更新方向相对稳定。

##### 更新公式

假设成本函数为MSE，梯度下降算法的更新公式为：

$$
\theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^m (\theta^T x_i - y_i) \cdot x_{ij} \right)
$$

[推倒过程](./梯度下降公式及正规方程推倒过程.md)

### 正规方程

> 
